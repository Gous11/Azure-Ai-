
Azure Open AI API vs Open AI API
1.Introduction of Azure openAl
2.Difference between Azure OpenAI API vs OpenAI API
3.Key functionalities of Azure Open Al
4. Azure Account Access Link
5. Azure Subscription Methods

Azure OpenAI Access
1.. How to take access of Azure OpenAl
2.Resource Creation
3.Model Deployment
4.Understanding of TPM and RPM
5.Chat Playground
6. Access of the different models Like GPT-3.5, GPT-4, Dalle-E, Whispe
7.Chat Completion API
8. Creating Custom Web App and Deployment of it.

Function calling using Azure OpenAl
1. Function calling using Azure OpenAl
2. How to use Python SDK
3. How to call third party API

Fine Tuning of AzureOpenAl Model
1.Select the model of Fine-Tuning
2.Preparing the data for Fine-Tuning
3. Cost Analysis of Fine-Tuning

Azure Al Studio
1. How to create Azure Al Hub and Resources.
2.How to Deploy the model
3.Create a RAG using Azure Al Studio
4. Streamline entire Pipeline using prompt flow
5. Create a Endpoint URL of your Prompt flow
6. Consum it in our Custom Application



# Azure OpenAI API vs OpenAI API

## 1. Introduction of Azure OpenAI

Azure OpenAI Service is a managed offering from Microsoft that provides access to OpenAI's advanced language models, such as GPT-4o, GPT-3.5-Turbo, DALL-E, and Whisper, integrated into the Azure cloud platform. It enables developers and organizations to build AI applications with enterprise-grade security, scalability, and compliance features. Unlike direct access through OpenAI, Azure OpenAI runs on Azure infrastructure, ensuring data isolation, private networking, and regional availability while adhering to responsible AI principles to mitigate risks like harmful content generation. Users interact with it via REST APIs, SDKs (Python, C#, JS, Java, Go), or the Azure AI Foundry portal playgrounds. It supports tasks like content generation, summarization, semantic search, and code translation, with built-in content filtering and virtual network integration for enhanced security.

## 2. Difference between Azure OpenAI API vs OpenAI API

Azure OpenAI and OpenAI APIs share core functionality but differ in deployment, security, and management. Here's a comparison:

| Aspect                  | Azure OpenAI API                                                                 | OpenAI API                                                                 |
|-------------------------|----------------------------------------------------------------------------------|----------------------------------------------------------------------------|
| **Hosting & Infrastructure** | Hosted on Azure with enterprise scalability, private endpoints, and VNet integration. | Hosted by OpenAI; no native Azure integration. |
| **Security & Compliance** | Built-in Azure security (e.g., encryption, RBAC, content filtering); data stays in Azure. | Basic API keys; optional DPA for GDPR, but less enterprise controls. |
| **API Compatibility** | Nearly identical to OpenAI's, with deployment names in URLs; supports same models. | Direct model names in requests; faster feature rollout (e.g., plugins). |
| **Pricing**             | Token-based + hourly hosting for deployments; PTUs for predictable costs. | Token-based only; tiered plans, generally cheaper without hosting fees. |
| **Data Handling**       | Uploads to Azure Storage; supports fine-tuning with isolation. | Limited fine-tuning; data processed externally. |
| **Access & Quota**      | Limited access via approval; TPM/RPM quotas per region/subscription. | Open access with rate limits; quicker model availability. |
| **Integration**         | Seamless with Azure services (e.g., Synapse, Power BI); managed identities. | Standalone; requires custom integration. |
| **Latency & Features**  | May lag in new features (e.g., 10 weeks for GPT-4); consistent embeddings. | Faster updates; variable latency without enterprise SLAs. |

Azure suits enterprises needing compliance and Azure ecosystem integration, while OpenAI is ideal for rapid prototyping.

## 3. Key Functionalities of Azure OpenAI

- **Model Access**: REST APIs and SDKs for GPT-4o, GPT-3.5-Turbo, Embeddings, DALL-E, Whisper; supports text, image, and audio tasks.
- **Security**: Private networking, content filtering, regional compliance (e.g., GDPR).
- **Customization**: Fine-tuning, RAG with Azure AI Search, function calling for external tools.
- **Integration**: With Azure services like Storage, Synapse; supports agents and event-triggered indexing.
- **Monitoring & Tools**: Playgrounds for testing, explainability, governance for responsible AI.
- **Scalability**: Provisioned Throughput Units (PTUs) for predictable performance; embeddings for semantic search.

## 4. Azure Account Access Link

To create an Azure account, visit [Azure Free Account](https://azure.microsoft.com/en-us/free/). This provides $200 in credits for 30 days and free tiers for select services (note: Azure OpenAI requires a paid subscription).

## 5. Azure Subscription Methods

Azure offers flexible subscription models:

| Method                  | Description                                                                 | Best For |
|-------------------------|-----------------------------------------------------------------------------|----------|
| **Free Account**        | $200 credit for 30 days; 12 months of free services (e.g., VMs, storage). No charge unless exceeding limits. | Testing and learning. |
| **Pay-As-You-Go**       | Pay for usage beyond free amounts; flexible scaling. | Individuals/small teams. |
| **Enterprise Agreement**| Volume licensing for large orgs; negotiated discounts. | Enterprises. |
| **Cloud Solution Provider (CSP)** | Resold via Microsoft partners; managed services. | SMBs needing support. |
| **MSDN/Visual Studio**  | Developer credits for testing. | Developers. |

Subscriptions act as billing/access boundaries; multiple per account for organization.

# Azure OpenAI Access

## 1. How to Take Access of Azure OpenAI

1. Create a paid Azure subscription (free tier doesn't support OpenAI).
2. Request access via the [Azure OpenAI Limited Access form](https://aka.ms/oai/access), providing company details (personal accounts limited). Approval takes 7-10 days.
3. Once approved, create resources in the Azure portal (search "Azure OpenAI").
4. Use API keys or managed identities for authentication.

## 2. Resource Creation

1. In Azure portal, search "Azure OpenAI" and select **Create**.
2. Basics: Subscription, resource group, region (e.g., East US), name, pricing tier (S0).
3. Network: Public or private access.
4. Review + Create.
CLI: `az cognitiveservices account create --name <name> --resource-group <rg> --kind OpenAI --sku S0 --location eastus`.

## 3. Model Deployment

1. In Azure OpenAI Studio (oai.azure.com), select your resource.
2. Go to **Deployments** > **Create new deployment**.
3. Choose model (e.g., gpt-4o-mini), deployment name, TPM quota.
4. Create; status changes to Succeeded.
CLI: Use `az cognitiveservices account deployment create`.

## 4. Understanding of TPM and RPM

- **TPM (Tokens Per Minute)**: Max tokens processed per minute per deployment/region/subscription. Based on estimated tokens in requests; e.g., 240K TPM allows ~1,440 RPM for GPT-3.5 (ratio: 1,000 TPM = 6 RPM).
- **RPM (Requests Per Minute)**: Derived from TPM; limits requests to prevent overload.
Quotas are subscription-scoped; request increases via support. Monitor via Azure portal.

## 5. Chat Playground

The Chat Playground in Azure AI Foundry portal is a no-code interface for testing models. Access via portal > Playground > Chat. Input prompts, select deployments (e.g., GPT-4o), and view responses with chain-of-thought summaries. Supports multimodal inputs (text, images, audio).

## 6. Access of the Different Models Like GPT-3.5, GPT-4, DALL-E, Whisper

Deploy models in Azure OpenAI Studio:
- **GPT-3.5/GPT-4**: Text generation; deploy gpt-35-turbo or gpt-4o.
- **DALL-E**: Image generation; deploy dall-e-3 (REST APIs GA, SDK preview).
- **Whisper**: Speech-to-text; deploy whisper-audio (limited regions).
Request access for previews (e.g., gpt-image-1). Use SDK/REST for inference.

## 7. Chat Completion API

Use for conversational AI. Endpoint: `POST https://{resource}.openai.azure.com/openai/deployments/{deployment}/chat/completions?api-version=2024-10-21`. Body: JSON with messages array, model deployment. Python SDK: `client.chat.completions.create(...)`. Supports streaming, tools.

## 8. Creating Custom Web App and Deployment of It

1. Build app (e.g., Node.js/Express) integrating Azure OpenAI SDK for chat/RAG.
2. Use managed identities for auth.
3. Deploy to Azure App Service: `az webapp up --runtime "NODE|18-lts"`.
4. Configure env vars (e.g., AZURE_OPENAI_ENDPOINT).
Test/deploy via VS Code or GitHub Actions.

# Function Calling Using Azure OpenAI

## 1. Function Calling Using Azure OpenAI

Define tools/functions in chat completions API (e.g., JSON schema for get_weather). Model returns tool calls with args. Execute function, feed back output for final response. Supports parallel calls; use `tool_choice` to force. Limits: 1,024 chars per description.

## 2. How to Use Python SDK

Install `openai`. Example:
```python
from openai import AzureOpenAI
client = AzureOpenAI(azure_endpoint="https://<resource>.openai.azure.com/", api_key="<key>", api_version="2024-07-01-preview")
tools = [{"type": "function", "function": {"name": "get_weather", "description": "...", "parameters": {...}}}]
response = client.chat.completions.create(model="gpt-4o", messages=[...], tools=tools)
```
Handle tool_calls, execute, resubmit.

## 3. How to Call Third Party API

Define function schema for API (e.g., weather API). Model generates args; execute via requests in Python, return JSON. Resubmit to model for response. Example: Integrate stock API for queries.

# Fine Tuning of Azure OpenAI Model

## 1. Select the Model of Fine-Tuning

Supported: GPT-4o-mini, GPT-3.5-Turbo. Choose based on task (e.g., chat vs. completion). Check regions (e.g., East US). Use Azure AI Foundry portal > Tools > Fine-tuning.

## 2. Preparing the Data for Fine-Tuning

Format as JSONL: `{"messages": [{"role": "system", ...}, {"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]}`. Min 10-100 examples; use validation set (10-20%). Upload via portal or CLI. Tools: OpenAI's `prepare_data` script for CSV/JSON conversion.

## 3. Cost Analysis of Fine-Tuning

- **Training**: $1.70-$8.00 per million tokens (input/output).
- **Hosting**: $0.41-$2.00/hour per deployment (even idle).
- **Inference**: Same as base models + hosting.
Example: Fine-tuning GPT-4o-mini on 1M tokens ~$4; hosting $1/day. Use Cost Management for monitoring; delete inactive deployments after 15 days.

# Azure AI Studio

## 1. How to Create Azure AI Hub and Resources

1. Azure portal > Search "Azure AI Foundry" > **+ Create** > Hub.
2. Basics: Name, subscription, RG, region.
3. AI Services: Select/create Azure OpenAI.
4. Optional: Storage, Key Vault, etc.
5. Review + Create. CLI: `az ml workspace create --name <hub> --resource-group <rg> --location eastus`.

## 2. How to Deploy the Model

In AI Foundry portal > Model catalog > Select model (e.g., gpt-4o) > **Use this model** > Deployment type (serverless/managed) > Create. Test in playground.

## 3. Create a RAG Using Azure AI Studio

1. Upload data to storage/index (Azure AI Search).
2. Create index with embeddings.
3. In Prompt Flow > Create flow > Add LLM + retrieval nodes.
4. Connect to deployed model; test with user query.

## 4. Streamline Entire Pipeline Using Prompt Flow

Prompt Flow orchestrates LLM calls, tools, and logic. Create flow: Portal > Prompt flow > New > Add nodes (LLM, Python, conditions). Evaluate with batch runs; iterate via traces.

## 5. Create a Endpoint URL of Your Prompt Flow

Deploy: Prompt flow > Select flow > **Deploy** > Online endpoint > Name, instance type. Get URL from Endpoints > Consume tab.

## 6. Consume It in Our Custom Application

Use REST: POST to endpoint URL with JSON payload (e.g., {"data": {"query": "..."}}). Auth: API key or Entra ID. Python example: `requests.post(url, headers={"Authorization": "Bearer <key>"}, json=payload)`. Integrate into apps like Streamlit or web services.




### What is RAG (Retrieval-Augmented Generation)? – Detailed Explanation

RAG is currently the most effective and widely adopted technique to make Large Language Models (LLMs) like GPT-4, GPT-4o, Llama 3, or Mistral give accurate, up-to-date, and domain-specific answers without expensive fine-tuning.

#### Core Idea in One Sentence
RAG = Retrieval (fetch relevant documents from your private data) + Augmentation (inject those documents into the prompt) + Generation (let the LLM generate the final answer using the retrieved context).

#### Why RAG Was Invented (The Problems It Solves)
| Problem with Pure LLMs                          | How RAG Solves It                                                                 |
|-------------------------------------------------|-----------------------------------------------------------------------------------|
| Knowledge cutoff (e.g., GPT-4 knows only up to 2023/2024) | Retrieves latest documents from your database/index                              |
| Hallucinations (making up facts)                | Grounds the answer in real retrieved text → dramatically reduces hallucinations |
| Cannot access private/company data              | You control the knowledge base (policies, product docs, CRM, etc.)               |
| Expensive and slow fine-tuning                  | No model retraining needed; just update the index                                 |
| Strict compliance & data privacy requirements   | Data never leaves your environment (especially important in Azure/OpenAI setup)  |

#### RAG Architecture – Step-by-Step (2025 Best Practices)

```
User Query
    ↓
1. Query Embedding (e.g., text-embedding-3-large)
    ↓
2. Vector Search → Top-k most relevant chunks (Azure AI Search, Pinecone, Weaviate, etc.)
    ↓
3. Retrieved chunks + original query → Stuffed into prompt (with system instructions)
    ↓
4. LLM (GPT-4o, GPT-4o-mini, etc.) generates final answer
    ↓
Answer (accurate + cited if needed)
```

#### Detailed Components

| Component                  | Purpose                                                                 | Popular Azure/Open-Source Options (2025)                                 |
|----------------------------|-------------------------------------------------------------------------|--------------------------------------------------------------------------|
| Document Ingestion         | Split PDFs, Word, HTML, DB exports into manageable chunks              | LangChain, LlamaIndex, Unstructured.io, Azure AI Document Intelligence  |
| Chunking Strategy          | Balance context window vs. precision                                   | 512–2048 tokens, overlap 20–50%, semantic chunking (better in 2025)     |
| Embedding Model            | Convert text → dense vector (1536 or 3072 dimensions)                  | text-embedding-3-large, text-embedding-ada-002, voyage-large-2, e5-large |
| Vector Database / Index    | Fast similarity search + metadata filtering                            | Azure AI Search (integrated vector + hybrid), Pinecone, Qdrant, PGVector |
| Metadata Filtering         | Filter by date, department, security level, language, etc.            | Essential for enterprise (e.g., only show docs user is authorized for)  |
| Re-ranking (optional)     | Improve precision of top-k using cross-encoder                         | Cohere Rerank 3, bge-reranker, FlashRank                                 |
| Prompt Template            | Instruct LLM to only use provided context and cite sources             | “You are a helpful assistant… Answer using only the following context…” |
| LLM                        | Final answer generation                                                | GPT-4o (best quality), GPT-4o-mini (cheapest & fast), Llama-3-70B, etc. |

#### Types of RAG (2025 Taxonomy)

| Type                     | Description                                                                                   | When to Use                                      |
|--------------------------|-----------------------------------------------------------------------------------------------|--------------------------------------------------|
| Naive RAG                | Simple retrieve → stuff → generate                                                            | Quick prototypes                                 |
| Advanced RAG             | Pre-retrieval (query rewriting) + post-retrieval (reranking, compression)                    | Most production systems                          |
| Modular RAG              | Separate modules: query classifier → different retrieval strategies → fusion → generation   | Complex enterprise apps                         |
| HyDE                     | Generate hypothetical answer first → use it as query for better retrieval                    | When query and document language differ          |
| Parent-Child / Hierarchical | Small chunks for retrieval + parent document for context                                      | Legal contracts, research papers                 |
| Knowledge Graph + RAG    | Combine vector search with structured KG queries                                              | Highly structured domains (finance, healthcare)  |
| Agentic RAG              | LLM decides whether to retrieve, which tools to call, multi-step reasoning                   | Customer support bots, research agents           |

#### Common RAG Failure Modes & Fixes (2025)

| Failure Mode                     | Symptom                                  | Fix (2025 Best Practice)                              |
|----------------------------------|------------------------------------------|-------------------------------------------------------|
| “Lost in the middle”             | Model ignores relevant chunk in position 15/30 | Put most relevant chunks at beginning & end         |
| Needle-in-haystack               | Relevant info buried in huge context     | Use reranking + context compression (LLMLingua, LongLLMLingua) |
| Outdated chunks                  | Index contains stale info                | Automated nightly/CDC-based re-indexing pipeline    |
| Topic drift                      | Retrieved irrelevant chunks              | Query expansion + reranking + better embeddings    |
| Hallucination despite context    | Model ignores provided context           | Stronger system prompt + “cite your sources” instruction |

#### Azure-Native RAG Stack (November 2025 – Recommended)

| Layer                        | Microsoft Service (2025)                                  | Why It Wins in Enterprise                                 |
|------------------------------|-----------------------------------------------------------|-----------------------------------------------------------|
| Document Ingestion           | Azure AI Document Intelligence + Logic Apps/Fabric        | Handles scanned PDFs, tables, handwriting                 |
| Chunking & Embedding         | Azure OpenAI embeddings (text-embedding-3-large)          | No egress of data, governed                               |
| Index                        | Azure AI Search (vector + hybrid search)                  | Built-in RBAC, IP filtering, semantic ranker, reranking   |
| Re-ranking (optional)        | Built-in semantic ranker or Cohere Rerank via marketplace| Native integration                                        |
| Orchestration                | Azure AI Studio Prompt Flow or Semantic Kernel            | Visual flow builder, tracing, evaluation, deployment      |
| LLM                          | Azure OpenAI GPT-4o / GPT-4o-mini                         | Private networking, content filters, quotas               |
| Monitoring                   | Azure Monitor + Content Safety + Application Insights    | Detect hallucinations, PII leaks, abuse                   |

#### Example Prompt Template (2025 Production-Grade)

```text
You are an expert assistant. Answer the question using ONLY the following context.
If the context does not contain enough information, say "I don't have sufficient information."

Context:
{retrieved_documents}

Question: {user_question}

Answer with citation like [1], [2] at the end of sentences where possible.
```

#### Performance & Cost Benchmarks (Nov 2025)

| Model + Setup                     | Latency (p90) | Cost per 1k queries | Accuracy (RAGAS score) |
|-----------------------------------|---------------|---------------------|------------------------|
| GPT-4o + Azure AI Search + rerank | ~1.8 s        | ~$11                | 92–96%                 |
| GPT-4o-mini + Azure AI Search     | ~0.9 s        | ~$2.10              | 88–93%                 |
| Llama-3-70B (self-hosted) + PGVector | ~2.2 s      | ~$4 (A100 cost)     | 90–94%                 |

#### When NOT to Use RAG
- Pure creative writing
- Simple Q&A where base LLM already knows the answer perfectly
- Real-time data <10 ms latency required (use smaller distilled models instead)

#### Summary – One-Page Mental Model
RAG turns an LLM from a “closed-book student” (only what it memorized) into an “open-book genius” that can look up your latest private documents in milliseconds and then synthesize a perfect, citation-backed answer.

It is the #1 reason most Fortune 500 companies in 2025 are successfully deploying production GenAI applications with Azure OpenAI.





# Enterprise Model Deployment Structure in Azure OpenAI

As of November 18, 2025, Azure OpenAI (integrated into Azure AI Foundry Models) provides a robust, scalable framework for deploying AI models in enterprise environments. This structure emphasizes isolation, compliance, multi-region support, and integration with Azure's ecosystem for security, monitoring, and cost management. Deploying models involves creating resources, selecting deployment types, and configuring for production workloads. Below, I'll break down the key components, architecture, best practices, and step-by-step guidance.

## Core Architecture Overview

Azure OpenAI deployments follow a **logical isolation model** where models are not "copied" per user but accessed via dedicated endpoints backed by shared infrastructure with strong tenancy boundaries. Key elements:

- **Resource Hierarchy**:
  - **Azure Subscription**: Billing and quota boundary. Enterprises often use multiple subscriptions for isolation (e.g., dev/test/prod).
  - **Resource Group**: Logical container for resources like Azure OpenAI instances.
  - **Azure OpenAI Resource**: The core service instance (created in a specific region). This is your "gateway" to models, with API endpoints like `https://{resource}.openai.azure.com/`.
  - **Model Deployments**: Specific instances of models (e.g., GPT-4o) within the resource. Each deployment has a unique name, quota (TPM/RPM), and configuration. You reference deployments (not model names) in API calls for isolation.

- **Isolation Levels** (Infrastructure, Platform, Application):
  | Level          | Description                                                                 | How Ensured in Azure OpenAI |
  |----------------|-----------------------------------------------------------------------------|-----------------------------|
  | **Infrastructure** | Physical/virtual separation of compute/storage.                            | Multi-tenant Azure VMs with hypervisor isolation; no shared kernels. Data at rest encrypted with customer-managed keys (CMK). |
  | **Platform**    | Network and access controls.                                               | Virtual Networks (VNets), Private Endpoints, RBAC via Microsoft Entra ID. Logical separation ensures calls don't cross tenants. |
  | **Application** | Model inference isolation.                                                 | Dedicated deployment slots; fine-tuned models are fully private. Prompts/responses never persist or train base models without opt-in. |

  Tenant isolation is guaranteed: Calls are logically segregated, and fine-tuned models are exclusive to your resource. For global enterprises, use a **Hub-and-Spoke topology**: Central "Hub" for shared governance (e.g., fine-tuning), "Spokes" for regional deployments.

- **Data Flow**:
  ```
  User/App → API Gateway (Azure API Management) → Private Endpoint → Azure OpenAI Resource → Deployment → Model Inference → Response
  ```
  Inference can route globally or regionally based on deployment type. Storage (e.g., for fine-tuning data) uses Azure Storage with geo-redundancy.

## Deployment Types: Tailoring for Enterprise Needs

Azure OpenAI offers flexible deployment strategies to balance cost, latency, and compliance. Choose based on workload: Standard for variable traffic, Provisioned for predictable high-volume.

| Deployment Type          | Description                                                                 | Best For | Throughput/Quota | Data Residency | Cost Model |
|--------------------------|-----------------------------------------------------------------------------|----------|------------------|----------------|------------|
| **Standard (Serverless)** | Pay-per-use; auto-scales. Sub-options: Global (dynamic routing), DataZone (Microsoft zones), Regional (specific Azure geo). | Prototyping, variable loads. | TPM-based (e.g., 30K default). | Varies by sub-option; Regional strongest. | Per-token; no idle costs. |
| **Provisioned (Managed)** | Reserved compute (PTUs); dedicated capacity. Global or Regional. | High-volume, low-latency (e.g., chatbots). | Fixed PTUs (e.g., 1K–1M); queues at 100% utilization. | Regional preferred for compliance. | Hourly + per-token; idle charges. |
| **Batch**                | Asynchronous processing for large jobs.                                    | Offline tasks (e.g., summarization). | TPM-scoped.     | Regional.     | Per-token. |
| **Developer (Fine-Tuned)** | Low-cost eval for custom models; no residency guarantee.                   | Testing fine-tunes. | Limited TPM.    | Global.       | Per-token. |

Flagship models (e.g., GPT-4o, GPT-4o-mini) support all types; previews may be limited. For enterprises, start with Regional Standard for compliance, scale to Provisioned for SLAs (99.9% uptime).

## Enterprise Best Practices

Leverage these for secure, scalable deployments:

1. **Governance & Security**:
   - Use Azure Policy for model access limits; integrate Microsoft Purview for data cataloging.
   - Enable Content Safety filters; monitor with Azure Defender for Cloud.
   - Multi-region redundancy: Deploy across 2+ regions (e.g., East US + West Europe) to avoid outages; use Azure Traffic Manager for failover.

2. **Scalability & Performance**:
   - Monitor utilization via Azure Monitor (e.g., 'Provisioned-Managed Utilization V2' metric); set alerts for 80% thresholds.
   - Auto-update deployments for latest model versions; use in-place migrations for zero-downtime (20-30 min).
   - Integrate with Azure Kubernetes Service (AKS) or API Management for traffic routing.

3. **Cost Optimization**:
   - Right-size PTUs; use spot instances for non-prod.
   - Automate cleanup of idle deployments (e.g., via Azure Logic Apps).
   - Track with Azure Cost Management; fine-tuning adds ~$1.70–$8/M tokens training + hourly hosting.

4. **MLOps Integration**:
   - Use Azure ML Pipelines or Prompt Flow for CI/CD; version models with Azure ML registry.
   - For fine-tuning: Hub for central training, Spokes for deployment; enforce expiration policies.

5. **Networking**:
   - Private Link for VNet integration; colocation reduces latency.

## Step-by-Step: Deploying a Model in Enterprise Setup

1. **Prerequisites**:
   - Paid Azure subscription with OpenAI quota (request via form if needed).
   - Access to Azure AI Foundry portal (ai.azure.com).

2. **Create Resource** (Hub/Spoke Hub):
   - Portal > Create > Azure OpenAI > Basics: Subscription, RG, Region (e.g., East US), Name, Pricing Tier (S0).
   - Network: Enable Private Endpoint if VNet-integrated.
   - Review + Create. CLI: `az cognitiveservices account create --name <name> --resource-group <rg> --kind OpenAI --sku S0 --location eastus`.

3. **Deploy Model**:
   - In AI Foundry > Model Catalog > Select (e.g., gpt-4o) > "Use this model".
   - Deployment: Type (e.g., Provisioned-Managed), Name (e.g., "prod-gpt4o"), PTUs/TPM.
   - Create. Status: Succeeded in ~5-10 min.
   - CLI: `az cognitiveservices account deployment create --name <resource> --resource-group <rg> --deployment-name <dep> --model-name gpt-4o --model-version latest --sku-capacity 1`.

4. **Test & Integrate**:
   - Playground: AI Foundry > Deployments > Chat.
   - API: Use SDK (e.g., Python: `AzureOpenAI(azure_endpoint=..., api_key=..., model="<deployment-name>")`).
   - Deploy to App: Use Azure App Service or AKS; expose via API Management.

5. **Monitor & Scale**:
   - Metrics: Portal > Resource > Metrics (Requests, Latency, Utilization).
   - Scale: Increase PTUs or add regions via new resources.

For multi-tenant: Use Entra ID for RBAC; separate deployments per tenant. This structure ensures enterprises can deploy GPT-4o at scale while meeting GDPR/SOX compliance. For custom needs, consult Azure Architecture Center for reference designs.